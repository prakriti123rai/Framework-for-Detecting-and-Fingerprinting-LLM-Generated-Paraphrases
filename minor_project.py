# -*- coding: utf-8 -*-
"""Minor_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ck8UqeU6Cn8fG8KBkk1wZImjBcNQ2yD7
"""

# ============================================================
# MASTER RE-INITIALIZATION CELL
# Run this whenever the runtime restarts
# ============================================================

import os
import pandas as pd
import numpy as np
import re
from collections import Counter
import json

# -----------------------------
# 1. Mount Google Drive
# -----------------------------
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# -----------------------------
# 2. Reload main dataset (df)
# -----------------------------
df = pd.read_csv("/content/drive/MyDrive/arpadpp/final_llm_detector_dataset.csv")
print("Dataset loaded:", df.shape)

# -----------------------------
# 3. Reload ARPaD results
# (Assuming you saved them earlier as JSON)
# If you used a different filename, update below
# -----------------------------

arpad_path = "/content/drive/MyDrive/arpadpp/arpad_results_full.json"

if os.path.exists(arpad_path):
    with open(arpad_path, "r") as f:
        arpad_results = json.load(f)
    print("ARPaD results loaded:", len(arpad_results))
else:
    print("⚠ ARPaD results file not found. Recompute ARPaD if needed.")

# -----------------------------
# 4. Reload spaCy model
# -----------------------------
import spacy
nlp = spacy.load("en_core_web_sm")
print("spaCy loaded.")

# -----------------------------
# 5. Reload Sentence-BERT model
# -----------------------------
from sentence_transformers import SentenceTransformer, util
sbert = SentenceTransformer('all-mpnet-base-v2')
print("SBERT loaded.")

# -----------------------------
# 6. Helper: Mask patterns
# -----------------------------
def mask_patterns(text, patterns):
    masked = text
    sorted_patts = sorted(patterns.keys(), key=lambda x: len(x.split()), reverse=True)
    for p in sorted_patts:
        escaped = re.escape(p)
        masked = re.sub(escaped, "[MASK]", masked)
    return " ".join(masked.split())

# -----------------------------
# 7. Helper: Semantic Shift Function
# -----------------------------
def semantic_shift_single(orig, para, patterns):
    masked_orig = mask_patterns(orig, patterns)
    masked_para = mask_patterns(para, patterns)

    e1 = sbert.encode(masked_orig, convert_to_tensor=True)
    e2 = sbert.encode(masked_para, convert_to_tensor=True)

    sim = float(util.cos_sim(e1, e2)[0][0])
    shift = 1 - sim

    return {
        "sim": sim,
        "shift": shift,
        "masked_orig": masked_orig,
        "masked_para": masked_para
    }

# -----------------------------
# 8. Lookup tables for original & paraphrase text
# -----------------------------
id_to_original   = df.set_index("id")["original_clean"].to_dict()
id_to_paraphrase = df.set_index("id")["paraphrase"].to_dict()

print("Re-initialization complete. You may resume processing.")
# ============================================================

import os
from google.colab import drive

# Ensure the mountpoint is empty before mounting
if os.path.exists('/content/drive'):
    # Attempt to unmount if it's already mounted
    # This part can sometimes be tricky if a previous mount failed partially
    try:
        drive.flush_and_unmount()
    except ValueError:
        pass # Ignore if not mounted
if os.path.isdir('/content/drive') and os.listdir('/content/drive'):
    import shutil
    shutil.rmtree('/content/drive')

os.makedirs('/content/drive', exist_ok=True)

drive.mount('/content/drive', force_remount=True)

# run this cell as a bash cell by prefixing with !
!pip install -q numpy pandas tqdm sentence-transformers transformers sacremoses sentencepiece spacy nltk
# download spacy model
!python -m spacy download en_core_web_sm

import os
# workspace in runtime
WORKDIR = "/content/arpadpp"
os.makedirs(WORKDIR, exist_ok=True)
# folder in your Drive to persist
DRIVE_DIR = "/content/drive/MyDrive/arpadpp"
os.makedirs(DRIVE_DIR, exist_ok=True)
print("Workdir:", WORKDIR)
print("Drive dir:", DRIVE_DIR)

import pandas as pd

# The original error indicates an issue with parsing the CSV, specifically
# "Expected 30 fields in line 7, saw 34". This often happens when the
# delimiter is not a comma, or when text fields containing the delimiter
# are not properly quoted.
# Given the file name 'bbc-news-data.csv', it's possible it's
# actually tab-separated or has some other non-standard formatting.
# Let's try specifying the delimiter as a tab, as this is a common issue
# with files that might be generated in spreadsheet software.
df = pd.read_csv("/content/drive/MyDrive/arpadpp/bbc-news-data.csv", sep='\t')
df.head()

import pandas as pd
import numpy as np
import re
import nltk
import spacy

nltk.download('punkt')
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

df = pd.read_csv("/content/drive/MyDrive/arpadpp/bbc-news-data.csv", sep='\t')
df.head()

# Basic cleaning function
#This cleans the text:
# removes line breaks
# removes multiple spaces
# removes weird characters

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.replace("\n", " ")
    text = text.strip()
    text = re.sub(r"\s+", " ", text)
    text = ''.join(c for c in text if c.isprintable())
    return text

df['clean_content'] = df['content'].apply(clean_text)
df[['title', 'clean_content']].head()

import pandas as pd
import numpy as np
import re
import nltk
import spacy

nltk.download('punkt')
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

df = pd.read_csv("/content/drive/MyDrive/arpadpp/bbc-news-data.csv", sep='\t')
df_sample = df.sample(500, random_state=42).reset_index(drop=True)
df_sample.head()

#Saving a copy
df_sample.to_csv("/content/clean_originals_sample.csv", index=False)

# TOKENIZATION
# We convert the cleaned text into tokens (words).
def tokenize(text):
    doc = nlp(text)
    return [t.text for t in doc if not t.is_space]

df_sample['clean_content'] = df_sample['content'].apply(clean_text)
df_sample['tokens'] = df_sample['clean_content'].apply(tokenize)
df_sample[['clean_content', 'tokens']].head()



!pip install transformers sentencepiece --quiet

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using:", device)

model_name = "google/flan-t5-large"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)

def paraphrase_t5(text, max_chunk_tokens=180):
    # 1. Split into chunks
    sentences = text.split(". ")
    chunks = []
    cur = ""

    for s in sentences:
        if len(tokenizer.encode(cur + s)) < max_chunk_tokens:
            cur += s + ". "
        else:
            if cur.strip():
                chunks.append(cur.strip())
            cur = s + ". "

    if cur.strip():
        chunks.append(cur.strip())

    # 2. Paraphrase each chunk
    par_chunks = []
    for chunk in chunks:
        prefix = "paraphrase: " + chunk
        input_ids = tokenizer.encode(prefix, return_tensors="pt", truncation=True).to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_length=200,
                num_beams=5,
                do_sample=False
            )

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        par_chunks.append(decoded)

    # 3. Join results
    return " ".join(par_chunks)

!ls -lh /content/drive/MyDrive/arpadpp/

partial_path = "/content/drive/MyDrive/arpadpp/pegasus_paraphrases.jsonl" # Corrected path to an existing file
with open(partial_path, "r") as f:
    lines = f.readlines()

len(lines)

import json

paraphrase_list = []
with open(partial_path, "r") as f:
    for line in f:
        paraphrase_list.append(json.loads(line))

print("Loaded:", len(paraphrase_list))

processed_ids = {item['id'] for item in paraphrase_list}

missing_df = df_sample[~df_sample['id'].isin(processed_ids)].reset_index(drop=True)
print("Remaining:", len(missing_df))

from tqdm import tqdm
import os

AUTOSAVE_EVERY = 10
count = len(paraphrase_list)

for idx, row in tqdm(missing_df.iterrows(), total=len(missing_df)):
    article_id = row['id']
    text = row['clean_content']

    try:
        para = paraphrase_t5(text)
    except Exception as e:
        print("Error:", e)
        continue

    paraphrase_list.append({
        "id": article_id,
        "original": text,
        "paraphrase": para,
        "generator": "t5-large"
    })

    count += 1

    # Autosave every 10 items
    if count % AUTOSAVE_EVERY == 0:
        with open(partial_path, "w") as f:
            for item in paraphrase_list:
                f.write(json.dumps(item) + "\n")
        print("Autosaved:", len(paraphrase_list))

with open("/content/drive/MyDrive/arpadpp/pegasus_paraphrases.jsonl") as f:
    lines = f.readlines()

len(lines)

import json
import pandas as pd

partial_path = "/content/drive/MyDrive/arpadpp/pegasus_paraphrases.jsonl"

para_list = []
with open(partial_path, "r") as f:
    for line in f:
        para_list.append(json.loads(line))

df_para = pd.DataFrame(para_list)
df_para.head()

!pip install -q google-generativeai

import google.generativeai as genai
genai.configure(api_key="AIzaSyCCJ_XUeSPJHJRM4KAx8HRjpwpZ-dqyXIk")

def paraphrase_gemini(text):
    model = genai.GenerativeModel("models/gemini-flash-latest")
    prompt = f"Paraphrase this text professionally without changing meaning:\n\n{text}"
    response = model.generate_content(prompt)
    return response.text.strip()

from tqdm import tqdm
import json, os

gemini_rows = []
N = 80  # change if needed
gemini_path = "/content/drive/MyDrive/arpadpp/gemini_paraphrases.jsonl"

for idx, row in tqdm(df_sample.head(N).iterrows(), total=N):
    try:
        para = paraphrase_gemini(row["clean_content"])
        gemini_rows.append({
            "id": str(row["id"]),
            "text": para,
            "generator": "gemini"
        })
    except Exception as e:
        print("Gemini Error:", e)
        continue

# SAVE
with open(gemini_path, "w") as f:
    for item in gemini_rows:
        f.write(json.dumps(item) + "\n")

print("Saved:", gemini_path)

!pip install -q transformers sentencepiece accelerate

from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

llama_model_name = "google/flan-t5-base"
llama_tok = T5Tokenizer.from_pretrained(llama_model_name)
llama_model = T5ForConditionalGeneration.from_pretrained(llama_model_name).to(device)

def paraphrase_llama(text):
    prefix = "paraphrase: " + text
    inputs = llama_tok(prefix, return_tensors="pt", truncation=True).to(device)

    with torch.no_grad():
        outputs = llama_model.generate(
            inputs["input_ids"],
            max_length=200,
            num_beams=5
        )
    return llama_tok.decode(outputs[0], skip_special_tokens=True)

llama_rows = []
llama_path = "/content/drive/MyDrive/arpadpp/llama_paraphrases.jsonl"
N = 100  # same number as gemini for balance

from tqdm import tqdm
for idx, row in tqdm(df_sample.head(N).iterrows(), total=N):
    try:
        para = paraphrase_llama(row["clean_content"])
        llama_rows.append({
            "id": str(row["id"]),
            "text": para,
            "generator": "llama"
        })
    except Exception as e:
        print("Llama Error:", e)
        continue

with open(llama_path, "w") as f:
    for item in llama_rows:
        f.write(json.dumps(item) + "\n")

print("Saved:", llama_path)

df_human = df_sample[['id','clean_content']].copy()
df_human['paraphrase'] = df_human['clean_content']
df_human['generator_model'] = 'human'
df_human = df_human.rename(columns={'clean_content':'original_clean'})

df_t5 = df_para[['id','original','paraphrase']].copy()
df_t5 = df_t5.rename(columns={'original':'original_clean'})
df_t5['generator_model'] = 'pegasis'
df_t5['id'] = df_t5['id'].astype(str)

df_gem = pd.DataFrame(gemini_rows)  # or load jsonl
df_gem = df_gem.rename(columns={'text':'paraphrase'})
df_gem['original_clean'] = df_gem['id'].map(df_sample.set_index('id')['clean_content'])
df_gem['generator_model'] = 'gemini'

df_llama = pd.DataFrame(llama_rows)  # or load jsonl
df_llama = df_llama.rename(columns={'text':'paraphrase'})
df_llama['original_clean'] = df_llama['id'].map(df_sample.set_index('id')['clean_content'])
df_llama['generator_model'] = 'llama'

df_final = pd.concat([
    df_human[['id','original_clean','paraphrase','generator_model']],
    df_t5[['id','original_clean','paraphrase','generator_model']],
    df_gem[['id','original_clean','paraphrase','generator_model']],
    df_llama[['id','original_clean','paraphrase','generator_model']]
], ignore_index=True)

print("==== BASIC SHAPE ====")
print(df_final.shape)

print("\n==== COLUMN CHECK ====")
print(df_final.columns.tolist())

# Expected columns: ['id','original_clean','paraphrase','generator_model']

print("\n==== NULL VALUES ====")
print(df_final.isna().sum())

print("\n==== CLASS DISTRIBUTION ====")
print(df_final['generator_model'].value_counts())

print("\n==== HUMAN SANITY CHECK (should match) ====")
df_hcheck = df_final[df_final['generator_model']=='human'].head(3)
for _, row in df_hcheck.iterrows():
    print("ID:", row['id'])
    print("Original:", row['original_clean'][:200])
    print("Paraphrase:", row['paraphrase'][:200])
    print("MATCH:", row['original_clean'][:200] == row['paraphrase'][:200])
    print("---")

print("\n==== T5 SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='pegasis'].head(3))

print("\n==== GEMINI SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='gemini'].head(3))

print("\n==== LLAMA SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='llama'].head(3))

print("\n==== CHECK UNIQUE IDs ====")
print("Unique IDs:", df_final['id'].nunique())
print("Total rows:", len(df_final))
print("Expected per-ID rows:", len(df_final) / df_final['id'].nunique())

df_final = df_final.drop_duplicates()

print("==== BASIC SHAPE ====")
print(df_final.shape)

print("\n==== COLUMN CHECK ====")
print(df_final.columns.tolist())

# Expected columns: ['id','original_clean','paraphrase','generator_model']

print("\n==== NULL VALUES ====")
print(df_final.isna().sum())

print("\n==== CLASS DISTRIBUTION ====")
print(df_final['generator_model'].value_counts())

print("\n==== HUMAN SANITY CHECK (should match) ====")
df_hcheck = df_final[df_final['generator_model']=='human'].head(3)
for _, row in df_hcheck.iterrows():
    print("ID:", row['id'])
    print("Original:", row['original_clean'][:200])
    print("Paraphrase:", row['paraphrase'][:200])
    print("MATCH:", row['original_clean'][:200] == row['paraphrase'][:200])
    print("---")

print("\n==== T5 SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='pegasis'].head(3))

print("\n==== GEMINI SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='gemini'].head(3))

print("\n==== LLAMA SAMPLE CHECK ====")
print(df_final[df_final['generator_model']=='llama'].head(3))

print("\n==== CHECK UNIQUE IDs ====")
print("Unique IDs:", df_final['id'].nunique())
print("Total rows:", len(df_final))
print("Expected per-ID rows:", len(df_final) / df_final['id'].nunique())

final_path = "/content/drive/MyDrive/arpadpp/final_llm_detector_dataset.csv"
df_final.to_csv(final_path, index=False)
print("Dataset saved to:", final_path)



for _, row in df_final.sample(5, random_state=42).iterrows():
    print("\nID:", row['id'])
    print("GENERATOR:", row['generator_model'])
    print("\nORIGINAL:")
    print(row['original_clean'][:300])
    print("\nPARAPHRASE:")
    print(row['paraphrase'][:300])
    print("\n" + "="*80)

import pandas as pd

path = "/content/drive/MyDrive/arpadpp/final_llm_detector_dataset.csv"

df = pd.read_csv(path)

df.head()

!pip install spacy --quiet
!python -m spacy download en_core_web_sm

!pip install sentence-transformers --quiet

import spacy
nlp = spacy.load("en_core_web_sm", disable=["parser","ner"])

# ================================
# ARPaD-style repeated pattern detector (3–15 word patterns)
# ================================
from collections import defaultdict, namedtuple
import math
import re

def clean_and_tokenize(text, nlp=None):
    """Clean text and tokenize into words."""
    if not isinstance(text, str):
        return []
    # Replace punctuation with space
    text = re.sub(r"[.,;!?:()\-\*\[\]\{\}\\\\/#\"]", " ", text)
    text = text.replace("\n", " ").strip()
    # Tokenize using spaCy if provided
    if nlp is not None:
        doc = nlp(text)
        toks = [t.text.lower() for t in doc if not t.is_space]
    else:
        toks = [t.lower() for t in text.split() if t.strip()]
    return toks

def build_suffixes(tokens, origin_label):
    """Build suffix tuples for all start positions."""
    suffixes = []
    for i in range(len(tokens)):
        suffixes.append((tuple(tokens[i:]), origin_label, i))
    return suffixes

def lcp_len(a, b, max_l=15):
    """Longest common prefix between suffix tuples."""
    m = min(len(a), len(b), max_l)
    c = 0
    for i in range(m):
        if a[i] == b[i]:
            c += 1
        else:
            break
    return c

def extract_shared_patterns(orig_tokens, para_tokens, min_l=3, max_l=15):
    """Find repeated patterns between original and paraphrase."""
    suf_orig = build_suffixes(orig_tokens, "orig")
    suf_para = build_suffixes(para_tokens, "para")
    combined = suf_orig + suf_para
    combined.sort(key=lambda x: x[0])  # lexicographic sort

    patterns = {}

    for i in range(len(combined)-1):
        s1, origin1, pos1 = combined[i]
        s2, origin2, pos2 = combined[i+1]
        if origin1 == origin2:
            continue  # must be cross-text match
        common_len = lcp_len(s1, s2, max_l=max_l)

        for L in range(min_l, common_len+1):
            patt = s1[:L]
            if patt not in patterns:
                patterns[patt] = {"occ_orig": [], "occ_para": []}
            if origin1 == "orig":
                patterns[patt]["occ_orig"].append(pos1)
                patterns[patt]["occ_para"].append(pos2)
            else:
                patterns[patt]["occ_orig"].append(pos2)
                patterns[patt]["occ_para"].append(pos1)

    # clean occurrence lists
    for p in patterns:
        patterns[p]["occ_orig"] = sorted(set(patterns[p]["occ_orig"]))
        patterns[p]["occ_para"] = sorted(set(patterns[p]["occ_para"]))

    return patterns

def compute_sim_ratios_from_patterns(orig_tokens, para_tokens, patterns_map, min_l=3, max_l=15):
    """Compute Sim_l for l = 3..15 using non-overlapping longest-first selection."""
    n_orig = len(orig_tokens)
    n_para = len(para_tokens)

    covered_orig = [False]*n_orig
    covered_para = [False]*n_para

    counts_shared_words = {L: 0 for L in range(min_l, max_l+1)}

    patt_by_len = defaultdict(list)
    for patt, occ in patterns_map.items():
        L = len(patt)
        if min_l <= L <= max_l:
            patt_by_len[L].append((patt, occ["occ_orig"], occ["occ_para"]))

    # longest-first
    for L in range(max_l, min_l-1, -1):
        for patt, occ_o, occ_p in patt_by_len[L]:

            used_para = set()
            matched = 0

            for opos in occ_o:
                if any(covered_orig[i] for i in range(opos, min(opos+L, n_orig))):
                    continue

                found = None
                for ppos in occ_p:
                    if ppos in used_para:
                        continue
                    if any(covered_para[j] for j in range(ppos, min(ppos+L, n_para))):
                        continue
                    found = ppos
                    break

                if found is not None:
                    # mark spans
                    for i in range(opos, min(opos+L, n_orig)):
                        covered_orig[i] = True
                    for j in range(found, min(found+L, n_para)):
                        covered_para[j] = True

                    used_para.add(found)
                    matched += 1

            counts_shared_words[L] += matched * L

    # compute similarity ratios
    sim_orig = {L: counts_shared_words[L] / n_orig if n_orig else 0 for L in counts_shared_words}
    sim_para = {L: counts_shared_words[L] / n_para if n_para else 0 for L in counts_shared_words}

    return sim_orig, sim_para, counts_shared_words

def arpad_pair_pipeline(original_text, paraphrase_text, nlp, min_l=3, max_l=15):
    """Full ARPaD pipeline for ONE pair."""
    orig_tokens = clean_and_tokenize(original_text, nlp)
    para_tokens = clean_and_tokenize(paraphrase_text, nlp)

    patterns = extract_shared_patterns(orig_tokens, para_tokens, min_l=min_l, max_l=max_l)

    sim_orig, sim_para, details = compute_sim_ratios_from_patterns(
        orig_tokens, para_tokens, patterns, min_l=min_l, max_l=max_l
    )

    patterns_str = { " ".join(p): patterns[p] for p in patterns }

    return {
        "orig_len": len(orig_tokens),
        "para_len": len(para_tokens),
        "patterns": patterns_str,
        "sim_orig": sim_orig,
        "sim_para": sim_para,
        "details": details
    }

row = df.iloc[0]
res = arpad_pair_pipeline(row["original_clean"], row["paraphrase"], nlp)
res

arpad_results = []

for idx, row in df.iterrows():
    result = arpad_pair_pipeline(
        row["original_clean"],
        row["paraphrase"],
        nlp
    )
    result["id"] = row["id"]
    result["generator_model"] = row["generator_model"]
    arpad_results.append(result)

len(arpad_results)

import pandas as pd

rows = []

for res in arpad_results:
    r = {
        "id": res["id"],
        "generator_model": res["generator_model"],
        "orig_len": res["orig_len"],
        "para_len": res["para_len"]
    }

    # Add similarity scores Sim_3 .. Sim_15
    for L, val in res["sim_orig"].items():
        r[f"Sim_{L}"] = val

    rows.append(r)

df_arpad = pd.DataFrame(rows)
df_arpad.head()

df_arpad.to_csv("/content/drive/MyDrive/arpadpp/arpad_features_full.csv", index=False)

from collections import Counter

all_patterns = Counter()

# Collect patterns from full arpad_results; gives unique multi-word patterns that were detected across the entire dataset
for res in arpad_results:
    for patt in res["patterns"].keys():
        all_patterns[patt] += 1

len(all_patterns)

from collections import Counter

pattern_df = Counter()

# Build pattern frequency PF (how many documents contain each pattern)
for res in arpad_results:
    for patt in res["patterns"].keys():
        pattern_df[patt] += 1

# Total number of unique patterns
print("Total unique patterns:", len(pattern_df))

# Preview first 20 patterns
sample_patterns = list(pattern_df.items())[:20]
sample_patterns

# Filter useful patterns (keep only those that appear in at least 5 documents)
MIN_DF = 5

filtered_patterns = {p: dfreq for p, dfreq in pattern_df.items() if dfreq >= MIN_DF}
print("Total filtered patterns:", len(filtered_patterns))

# PREVIEW first 20 filtered patterns
list(filtered_patterns.items())[:20]

# Compute IDF for each filtered pattern
#Patterns that appear everywhere → low IDF (not informative)
#Patterns that are rare → high IDF (very informative)
import math

idf = {}

for patt, dfreq in filtered_patterns.items():
   #N= total number of documents in the dataset, DF = the number of documents (rows) in which a pattern appears.
   # IDF = log(N / DF)
    idf[patt] = math.log(N / dfreq)

# Preview first 20 IDF values
list(idf.items())[:20]

#Compute PF-IDF
#High PF-IDF means: the pattern appears in several document, but not too many, so it is rare but still useful
pfidf = {}

for patt, dfreq in filtered_patterns.items():
    pfidf[patt] = dfreq * idf[patt]

# Preview first 20 PF-IDF values
list(pfidf.items())[:20]

#Per-document PF-IDF features
#pfidf_sum → total importance of preserved patterns in that document
#pfidf_max → strongest rare preserved pattern
#pfidf_mean → average importance
pfidf_rows = []

for res in arpad_results:
    doc_pfidf = []

    for patt in res["patterns"].keys():
        if patt in pfidf:
            doc_pfidf.append(pfidf[patt])

    if doc_pfidf:
        row = {
            "id": res["id"],
            "generator_model": res["generator_model"],
            "pfidf_sum": sum(doc_pfidf),
            "pfidf_max": max(doc_pfidf),
            "pfidf_mean": sum(doc_pfidf)/len(doc_pfidf)
        }
    else:
        row = {
            "id": res["id"],
            "generator_model": res["generator_model"],
            "pfidf_sum": 0,
            "pfidf_max": 0,
            "pfidf_mean": 0
        }

    pfidf_rows.append(row)

df_pfidf = pd.DataFrame(pfidf_rows)
df_pfidf.head()

#Save PF-IDF features
df_pfidf.to_csv("/content/drive/MyDrive/arpadpp/pfidf_features_full.csv", index=False)
print("PF-IDF features saved!")

# Sentence-BERT for semantic similarity measurement
from sentence_transformers import SentenceTransformer, util

# Using a compact but strong model
sbert = SentenceTransformer('all-mpnet-base-v2')

#Mask Patterns From Text Before Semantic Comparison
import re

def mask_patterns(text, patterns):
    """
    Mask all ARPaD-detected shared patterns from a given text.

    Why?
    --------
    We want semantic similarity ONLY on the rewritten / changed parts
    and NOT on repeated surface patterns.

    Parameters:
    - text (str): Original or paraphrased text
    - patterns (dict): Dictionary from ARPaD containing all patterns

    Returns:
    - masked_text (str): text where all repeated n-grams are replaced with [MASK]
    """
    masked = text
    for p in patterns.keys():
        # Regex escape required to avoid special char issues
        escaped = re.escape(p)
        masked = re.sub(escaped, "[MASK]", masked)
    return masked

#Compute Semantic Shift for a Single Row
import torch
from sentence_transformers import util

# Lists to store masked texts and metadata
masked_origs = []
masked_paras = []
ids = []
gens = []

print("Preparing masked texts...")

for res in arpad_results:
    rid = res["id"]

    orig = id_to_original[rid]
    para = id_to_paraphrase[rid]
    patterns = res["patterns"]

    # Mask ARPaD patterns from both texts
    masked_o = mask_patterns(orig, patterns)
    masked_p = mask_patterns(para, patterns)

    masked_origs.append(masked_o)
    masked_paras.append(masked_p)
    ids.append(rid)
    gens.append(res["generator_model"])

print("Masked texts ready:", len(masked_origs))

# -------------------------------------
#  SBERT batched embedding
# -------------------------------------

print("Encoding originals in batches...")
e_orig = sbert.encode(
    masked_origs,
    convert_to_tensor=True,
    batch_size=32,     # batches of 32 → very fast
    show_progress_bar=True
)

print("Encoding paraphrases in batches...")
e_para = sbert.encode(
    masked_paras,
    convert_to_tensor=True,
    batch_size=32,
    show_progress_bar=True
)
#  Compute cosine similarity (vectorized)
print("Computing cosine similarities...")
cos_matrix = util.cos_sim(e_orig, e_para)

# extract diagonal (pairwise similarity)
cos_sims = cos_matrix.diagonal().cpu().numpy()

# semantic shift
semantic_shifts = 1 - cos_sims
#  Build Results DataFrame
df_semantic = pd.DataFrame({
    "id": ids,
    "generator_model": gens,
    "semantic_similarity": cos_sims,
    "semantic_shift": semantic_shifts
})
df_semantic.head()

save_path = "/content/drive/MyDrive/arpadpp/semantic_similarity_shift.csv"
df_semantic.to_csv(save_path, index=False)
print("Semantic similarity + shift saved to:", save_path)

#Merge ALL FEATURE SETS (Master Integration)
import pandas as pd

df_main = pd.read_csv("/content/drive/MyDrive/arpadpp/final_llm_detector_dataset.csv") # Main dataset
df_pfidf = pd.read_csv("/content/drive/MyDrive/arpadpp/pfidf_features_full.csv")       # PF-IDF features
df_sem = pd.read_csv("/content/drive/MyDrive/arpadpp/semantic_similarity_shift.csv")   # Semantic-shift features
df_arpad = pd.read_csv("/content/drive/MyDrive/arpadpp/arpad_features_full.csv") # ARPaD similarity features

# sanity check: print columns & dtypes & a few rows
for name, df_obj in [("df_main", df), ("df_arpad", df_arpad), ("df_pfidf", df_pfidf), ("df_sem", df_sem)]:
    print("===", name, "===")
    print("columns:", df_obj.columns.tolist())
    print(df_obj.dtypes)
    display(df_obj.head(3))
    print()

# strip column names (removes leading/trailing spaces, invisible chars)
for df_obj in [df, df_arpad, df_pfidf, df_sem]:
    df_obj.columns = [c.strip() if isinstance(c, str) else c for c in df_obj.columns]

# normalize id -> string, generator_model -> stripped lowercase (or keep case if you prefer)
for name, df_obj in [("df_main", df), ("df_arpad", df_arpad), ("df_pfidf", df_pfidf), ("df_sem", df_sem)]:
    # id
    if 'id' in df_obj.columns:
        df_obj['id'] = df_obj['id'].astype(str).str.strip()
    # generator_model
    if 'generator_model' in df_obj.columns:
        df_obj['generator_model'] = df_obj['generator_model'].astype(str).str.strip()
    print(name, "-> ids:", df_obj['id'].nunique(), "rows:", len(df_obj))

# if any df is missing the column, try to bring it from main df (fallback)
if 'generator_model' not in df_arpad.columns:
    print("df_arpad missing generator_model -> attempting to map from df")
    df_arpad['generator_model'] = df_arpad['id'].map(df.set_index('id')['generator_model'])

if 'generator_model' not in df_pfidf.columns:
    print("df_pfidf missing generator_model -> attempting to map from df")
    df_pfidf['generator_model'] = df_pfidf['id'].map(df.set_index('id')['generator_model'])

if 'generator_model' not in df_sem.columns:
    print("df_sem missing generator_model -> attempting to map from df")
    df_sem['generator_model'] = df_sem['id'].map(df.set_index('id')['generator_model'])

# now show NaN counts
for name, df_obj in [("df_main", df), ("df_arpad", df_arpad), ("df_pfidf", df_pfidf), ("df_sem", df_sem)]:
    print(name, "generator_model NaNs:", df_obj['generator_model'].isna().sum())

# start from main df (id + generator_model)
df_main = df[['id', 'generator_model']].copy()

# merge ARPaD
m1 = df_main.merge(df_arpad, on=['id','generator_model'], how='left', indicator='arpad_merge_flag')
print("After merge arpad:", m1['arpad_merge_flag'].value_counts())

# merge PF-IDF
m2 = m1.merge(df_pfidf, on=['id','generator_model'], how='left', indicator='pfidf_merge_flag')
print("After merge pfidf:", m2['pfidf_merge_flag'].value_counts())

# merge semantic
m3 = m2.merge(df_sem, on=['id','generator_model'], how='left', indicator='semantic_merge_flag')
print("After merge semantic:", m3['semantic_merge_flag'].value_counts())

# final shape
print("final merged shape:", m3.shape)
display(m3.head())

# canonicalization example (only if you find mismatches)
# map common typos
mapping = {'pegasis':'pegasus', 'pegasis ':'pegasus'}  # add if needed
for df_obj in [df, df_arpad, df_pfidf, df_sem]:
    df_obj['generator_model'] = df_obj['generator_model'].replace(mapping)
    # or lower-case all
    df_obj['generator_model'] = df_obj['generator_model'].str.lower().str.strip()

# Save final merged dataset
final_merged = m3.drop(columns=['arpad_merge_flag','pfidf_merge_flag','semantic_merge_flag']) # drop indicator columns
final_merged.to_csv("/content/drive/MyDrive/arpadpp/final_merged_features.csv", index=False)
print("Saved:", "/content/drive/MyDrive/arpadpp/final_merged_features.csv")

import pandas as pd

path_feat = "/content/drive/MyDrive/final_merged_features.csv"
df_feat = pd.read_csv(path_feat)

df_feat.head()
df_feat.shape

from sklearn.preprocessing import LabelEncoder

# 1. Target: generator_model → numeric labels
le = LabelEncoder()
df_feat["label"] = le.fit_transform(df_feat["generator_model"])

print("Classes:", le.classes_)

# 2. Feature matrix X: drop non-feature columns
non_feature_cols = ["id", "generator_model", "label"]
feature_cols = [c for c in df_feat.columns if c not in non_feature_cols]

X = df_feat[feature_cols].values
y = df_feat["label"].values

X.shape, y.shape

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y  # keep class balance in train & test
)

X_train.shape, X_test.shape

!pip install -q xgboost

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np

class KLDClassifier(BaseEstimator, ClassifierMixin):
    """
    Simple KLD-based classifier for 'LLM fingerprinting'.
    For each class c, store a mean feature vector mu_c.
    Turn mu_c into a probability distribution q_c via softmax.
    For a sample x, turn x into p_x via softmax and pick class
    with smallest D_KL(p_x || q_c).
    """
    def __init__(self, eps=1e-9):
        self.eps = eps  # smoothing
        self.class_means_ = None
        self.classes_ = None

    def _softmax(self, v):
        v = np.asarray(v, dtype=float)
        v = v - np.max(v)  # numerical stability
        exp_v = np.exp(v)
        return exp_v / (np.sum(exp_v) + self.eps)

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y)
        self.classes_ = np.unique(y)

        self.class_means_ = {}
        for c in self.classes_:
            X_c = X[y == c]
            mu_c = X_c.mean(axis=0)
            q_c = self._softmax(mu_c)
            self.class_means_[c] = q_c
        return self

    def _kl_div(self, p, q):
        # D_KL(p || q)
        p = np.asarray(p) + self.eps
        q = np.asarray(q) + self.eps
        return np.sum(p * np.log(p / q))

    def predict(self, X):
        X = np.asarray(X, dtype=float)
        preds = []
        for x in X:
            p_x = self._softmax(x)
            # compute KL to each class fingerprint
            dists = []
            for c in self.classes_:
                q_c = self.class_means_[c]
                d = self._kl_div(p_x, q_c)
                dists.append(d)
            # pick class with minimum KL
            c_hat = self.classes_[np.argmin(dists)]
            preds.append(c_hat)
        return np.array(preds)

    def predict_proba(self, X):
        """
        Convert KL distances into similarity scores, then normalize.
        Lower KL => higher similarity.
        """
        X = np.asarray(X, dtype=float)
        all_probs = []
        for x in X:
            p_x = self._softmax(x)
            dists = []
            for c in self.classes_:
                q_c = self.class_means_[c]
                d = self._kl_div(p_x, q_c)
                dists.append(d)
            dists = np.array(dists)
            # similarity = exp(-d)
            sims = np.exp(-dists)
            probs = sims / (np.sum(sims) + self.eps)
            all_probs.append(probs)
        return np.vstack(all_probs)

models = {}

# 1) Logistic Regression (multinomial)
models["LogisticRegression"] = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(
        multi_class="multinomial",
        max_iter=2000,
        n_jobs=-1
    ))
])

# 2) Random Forest
models["RandomForest"] = RandomForestClassifier(
    n_estimators=400,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    n_jobs=-1,
    random_state=42
)

# 3) XGBoost
models["XGBoost"] = XGBClassifier(
    n_estimators=400,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.9,
    objective="multi:softprob",
    eval_metric="mlogloss",
    random_state=42,
    n_jobs=-1
)

# 4) SVM (RBF kernel)
models["SVM_RBF"] = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(
        kernel="rbf",
        probability=True,
        C=2.0,
        gamma="scale",
        random_state=42
    ))
])

# 5) KLD-based fingerprint classifier
models["KLDClassifier"] = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", KLDClassifier())
])

models.keys()

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix,
    precision_score,
    recall_score
)

results = []

for name, model in models.items():
    print("="*90)
    print(f"Training model: {name}")

    # Train model
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # ---- MAIN METRICS ----
    acc   = accuracy_score(y_test, y_pred)
    f1m   = f1_score(y_test, y_pred, average="macro")       # Macro F1
    f1w   = f1_score(y_test, y_pred, average="weighted")    # Weighted F1
    precm = precision_score(y_test, y_pred, average="macro")
    recm  = recall_score(y_test, y_pred, average="macro")

    # ---- PRINT RESULTS ----
    print(f"\nAccuracy:               {acc:.4f}")
    print(f"Macro Precision:        {precm:.4f}")
    print(f"Macro Recall:           {recm:.4f}")
    print(f"Macro F1-score:         {f1m:.4f}")
    print(f"Weighted F1-score:      {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    print("Confusion Matrix (rows = true, cols = predicted):")
    print(confusion_matrix(y_test, y_pred))

    # ---- SAVE RESULTS FOR SUMMARY ----
    results.append({
        "model": name,
        "accuracy": acc,
        "precision_macro": precm,
        "recall_macro": recm,
        "macro_f1": f1m,
        "weighted_f1": f1w
    })

# ---- PRINT SUMMARY TABLE ----
print("\nSUMMARY (sorted by macro F1-score):")
results_sorted = sorted(results, key=lambda d: d["macro_f1"], reverse=True)

for r in results_sorted:
    print(f"{r['model']}: "
          f"acc={r['accuracy']:.4f}, "
          f"prec_macro={r['precision_macro']:.4f}, "
          f"recall_macro={r['recall_macro']:.4f}, "
          f"macro_f1={r['macro_f1']:.4f}, "
          f"weighted_f1={r['weighted_f1']:.4f}")

import pandas as pd

# Convert list of dicts -> DataFrame
results_df = pd.DataFrame(results)
results_df

import matplotlib.pyplot as plt
import numpy as np

# Make sure results_df exists
display(results_df)

models = results_df["model"].tolist()
accuracy = (results_df["accuracy"] * 100).tolist()        # to %
macro_f1 = (results_df["macro_f1"] * 100).tolist()        # to %

# --- Plot 1: Accuracy (%)
plt.figure(figsize=(8, 4))
plt.bar(models, accuracy)
plt.ylabel("Accuracy (%)")
plt.xlabel("Model")
plt.title("Model Accuracy Comparison")
plt.xticks(rotation=30)
plt.ylim(0, 100)
plt.tight_layout()
plt.show()

# --- Plot 2: Macro F1 (%)
plt.figure(figsize=(8, 4))
plt.bar(models, macro_f1)
plt.ylabel("Macro F1-score (%)")
plt.xlabel("Model")
plt.title("Model Macro F1-score Comparison")
plt.xticks(rotation=30)
plt.ylim(0, 100)
plt.tight_layout()
plt.show()

metrics = ["accuracy", "precision_macro", "recall_macro", "macro_f1", "weighted_f1"]

# Convert to percentage for readability
metrics_percent = results_df.copy()
for m in metrics:
    metrics_percent[m] = metrics_percent[m] * 100

# --- One plot per metric (clean & simple) ---
for m in metrics:
    plt.figure(figsize=(8, 4))
    plt.bar(metrics_percent["model"], metrics_percent[m])
    plt.ylabel(f"{m} (%)")
    plt.xlabel("Model")
    plt.title(f"{m} Comparison Across Models")
    plt.xticks(rotation=30)
    plt.ylim(0, 100)
    plt.tight_layout()
    plt.show()